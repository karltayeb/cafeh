{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from spike_and_slab_ser import SpikeSlabSER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Example\n",
    "\n",
    "Here we simulate summary statistics (zscores) on ~2000 SNPs across 10 tissues.\n",
    "\n",
    "There are 4 causal SNPs in the sumation active it tissues[0-1], [2-5], [4-8], [0-8] respectively. The final tissue has no causal SNPs active in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma, causal_snps, tissue_membership, causal = pickle.load(open('../simulation_scripts/T10_simulation', 'rb'))\n",
    "T, N = causal.shape\n",
    "\n",
    "effectsize = 5  # zscore @ causal snps\n",
    "\n",
    "Sigma_reg = (Sigma + np.eye(N)*1e-6) / (1+1e-6)\n",
    "chol = np.linalg.cholesky(Sigma_reg)\n",
    "\n",
    "# simulate data according to MVN distribution of marignal test statistics\n",
    "Y = (effectsize * Sigma @ causal.T + chol @ np.random.normal(size=causal.T.shape)).T\n",
    "X = Sigma_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "There are three parameters you need to specify.\n",
    "1. `K` is the max number of components.\n",
    "2. `prior_activity` is an array of length `K` with the prior probability that a tissues is active in each component. \n",
    "3. `prior_variance` is the prior variance on the entries of the weight matrix. This parameters can have a large impact on the performance of the algorithm. The variation approximmation $q(\\{w_{tk}, s_{tk}\\})$ is a mixture of two gaussians-- an approximate posterior to $p(w_{tk} | s_{tk} = 1)$ and $p(w_{tk} | s_{tk} = 0) = p(w_{tk})$. It turns out that the variance of the first distribution is determined by `prior_variance`-- small setting will result in harder assignments\n",
    "\n",
    "There are two pieces of data you need to provide to the model \n",
    "1. `X` is the $N \\times N$ LD matrix\n",
    "2. `Y` is a $T \\times N$ matrix of z-scores for each SNP in T tissues\n",
    "\n",
    "You can also provide labels $\\text{snp_ids}$ and $\\text{tissue_ids}$ that will make the output of the model easier to read\n",
    "\n",
    "There is no reason the model can't support NAN entries in $Y$ but that needs to implemented\n",
    "\n",
    "\n",
    "### Training the model\n",
    "\n",
    "To train the model we use a forward selection scheme on the variational parameters. That is, we first train the model as if there were only one component with various initializations. We select the best model in terms of the evidence lower bound. We then train the model as through there were two components with various initializations, and again select the model with the best ELBO. We proceed until we are learning the full model with K models.\n",
    "\n",
    "This approach is always increasing in the elbo. In the lth step we are simply performing coordinate ascent on the variaional parameters except that the last $(K-l)$ components are  fixed at a setting where they are irrelevant/inactive. While this is considerably slower than training everything at once, we hope it avoids a lot of poor local optima. Specifically, we hope this avoids the scenario where two components capture the same effect. At a forward step, the elbo will prefer a model that describes new areas of the data to an model that captures the same effect.\n",
    "\n",
    "We also think this is necessary since the terminal state of the component is extremely sensitive to initialization of the weights. The simple solution is that for each initialization we assign a weight of 1 to one tissue and 0 to the others-- this initialization is destined to find a component that includes the active tissue. Do this across all tissues and select the best initialization.\n",
    "\n",
    "`forward_fit(early_stop=True)` uses the forward selection type optimization scheme as described above. For now, I suggest using this as it seems to give the type of results we want. `early_stop` is an option that, if at a step of optimization you learn that a component is essentially inactive, we will zero out assigments for all future components and only fit the model one final time.\n",
    "\n",
    "`fit()` just trains the model from one random initialization. It is much faster, but the results are much less stable since this model is quite sensitive to initialization of `weights` and `activity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8\n",
    "prior_activity = np.exp(-1*np.linspace(3, 3, K))\n",
    "prior_variance = 5.0\n",
    "\n",
    "model = SpikeSlabSER(\n",
    "    X=X, Y=Y, K=K,\n",
    "    snp_ids=np.arange(N), tissue_ids=np.arange(T),\n",
    "    prior_activity=prior_activity,\n",
    "    prior_variance=prior_variance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.forward_fit(early_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Example call of different plotting functions\n",
    "\n",
    "`plot_component(thresh=0.1)` plots the probability of using a component, weights of each component per tissue, and the posterior categorical distributions of each active component, components which have $p(\\text{tissue t active in component k}) > \\text{thresh}$ for some tissue\n",
    "\n",
    "`plot_predictions()` in the first row superimposes the observed z scores (black crosses) and expected value under the trained model (red circles), in the second row plots predictions against observed z scores\n",
    "\n",
    "`plot_manhattan(component, thresh=0.0)` makes manhattan plots for a component, colored by r^2 with the top snp in that component. `thresh` is a minimum probability to include a tissue in the plot. Tissues are plotted in decreasing order of probability of using `component`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_manhattan(component=0, thresh=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at inducing points\n",
    "\n",
    "Each component squeezes the data through one SNP. With a linear kernel we can think of this as a linear regression over SNPs with a single feature-- the correlation with some other SNP $z$. We put a categorical prior on $z$. Looking at the posterior shows us likely causal SNPs for the signal captured by that component\n",
    "\n",
    "`get_top_snp_per_component()` returns the top SNP per component and the probability assigned to that snp\n",
    "\n",
    "`get_confidence_sets(alpha=0.9, thresh=0.1)` returns, for each component, the top weighted snps so that $\\sum p_i > \\alpha$, 'thresh' filters out components with maximum assignment probability $ < \\text{thresh}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "top_snps, top_snp_probabilities = model.get_top_snp_per_component()\n",
    "print('True causal snps {}\\nTop SNP per component {}\\nProbability of top SNP: {}'.format(causal_snps, top_snps, top_snp_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = model.get_confidence_sets(0.99, thresh=0.5)\n",
    "print('\\n'.join(['component {}: {}'.format(key, cs[key]) for key in cs.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocalization\n",
    "\n",
    "`get_component_colocalization(component)` returns the probability under the model that two tissues both use `component`\n",
    "\n",
    "`get_global_colocalization` returns the probability that pairs of tissues share at least one component in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_component_colocalization(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_global_colocalzation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real eQTL example\n",
    "\n",
    "Now we show a quick run through a real eQTL exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(zscore_path, ld_path, gene):\n",
    "    \"\"\"\n",
    "    small helper function to read in data\n",
    "    we also regulize our estimate of X by adding a small positive to diagonal and renormalizing\n",
    "    this shouldnt do much to change the shape of the data distribution, just make it full rank\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(ld_path + gene, index_col=0)\n",
    "    zscores = pd.read_csv(zscore_path + gene + '.zscore_matrix.txt', '\\t', index_col=0)\n",
    "\n",
    "    nan_snps = np.all(np.isnan(X.values), axis=1)\n",
    "    X = X.iloc[~nan_snps].iloc[:, ~nan_snps]\n",
    "\n",
    "    active_snps = np.isin(X.index, zscores.index)\n",
    "    X = X.iloc[active_snps].iloc[:, active_snps]\n",
    "\n",
    "    active_snps = np.isin(zscores.index, X.index)\n",
    "    Y = zscores.iloc[active_snps]\n",
    "    Y = Y.iloc[:, ~np.any(np.isnan(Y.values), 0)]\n",
    "    \n",
    "\n",
    "    tissues = Y.columns.values\n",
    "    snp_ids = Y.index.values\n",
    "    pos = np.array([int(snp_id.split('_')[1]) for snp_id in snp_ids])\n",
    "\n",
    "    Y = Y.T.values\n",
    "    X = X.values\n",
    "    X = (X + np.eye(X.shape[0])*1e-6) / (1+1e-6)\n",
    "    \n",
    "    return X, Y, tissues, snp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'ENSG00000073464.11'\n",
    "#gene = 'ENSG00000141644.17'\n",
    "#gene = 'ENSG00000164904.17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_path = '../marios_correlation_matrices/'\n",
    "zscoore_path = '../data/zscore_genes_for_Karl/'\n",
    "\n",
    "X, Y, tissues, snp_ids = get_inputs(zscoore_path, ld_path, gene)\n",
    "T, N = Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "K = 20\n",
    "prior_activity = np.exp(-1*np.linspace(3, 10, K))\n",
    "model = SpikeSlabSER(X, Y, K, snp_ids, tissues, prior_activity, 10.0)\n",
    "\n",
    "model.weights = np.zeros_like(model.weights)\n",
    "model.forward_fit(early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_assignment_kl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active[:, 13].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_confidence_sets(thresh=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_confidence_sets_ld(thresh=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.elbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_manhattan(component=0, thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_colocs = {}\n",
    "for k in range(K):\n",
    "    probs = (model.active[:, k] * model.pi[:, k][:, None])\n",
    "    coloc = np.zeros((T, T))\n",
    "    for t1 in range(T):\n",
    "        for t2 in range(T):\n",
    "            coloc[t1, t2] = 1 - np.exp(np.sum(np.log(1 - probs[:, t1] * probs[:, t2])))\n",
    "            coloc[t1, t2] = np.sum(1 - probs[:, t1] * probs[:, t2])\n",
    "    component_colocs[k] = pd.DataFrame(coloc, index=model.tissue_ids, columns=model.tissue_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "np.stack([np.outer(model.active[:, k], model.active[:, k]) for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "componentwise = model.get_component_colocalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - np.exp(np.sum(np.log(1 - componentwise), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_pip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "snp = 4\n",
    "\n",
    "pip = np.zeros((N, T))\n",
    "for t in range(T):\n",
    "    for n in range(N):\n",
    "        pip[n, t] = 1 - np.exp(np.sum(np.log(1 - model.pi[n] * model.active[t])))\n",
    "        \n",
    "return pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pip[pip.max(1) > 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights.shape, np.isnan(model.Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_manhattan(component=3, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_component_colocalization(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_global_colocalzation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
