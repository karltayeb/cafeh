{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,8)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixed_ser import *\n",
    "from train_mixed_ser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma, causal_snps, tissue_membership, causal = pickle.load(open('../gp_fine_mapping/simulation_scripts/T10_simulation', 'rb'))\n",
    "T, N = causal.shape\n",
    "\n",
    "effectsize = 8\n",
    "\n",
    "Sigma_reg = (Sigma + np.eye(N)*1e-6) / (1+1e-6)\n",
    "chol = np.linalg.cholesky(Sigma_reg)\n",
    "Y = (effectsize * Sigma @ causal.T + chol @ np.random.normal(size=causal.T.shape)).T\n",
    "X = Sigma_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='output/mixed_ser'\n",
    "name = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "penalty = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 100\n",
    "max_restart = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# initialize (variational) parameters #\n",
    "#######################################\n",
    "pi = np.random.random((N, K)) + 1\n",
    "pi = pi / pi.sum(0)\n",
    "\n",
    "beta_means = np.random.random(K)\n",
    "beta_vars = np.ones(K)\n",
    "weights = np.random.random((T, K)) + 1\n",
    "\n",
    "problem, param_dict = make_problem(N, K, True)\n",
    "\n",
    "#########################\n",
    "# make mvn distribution #\n",
    "#########################\n",
    "\n",
    "dist = multivariate_normal(cov=X)\n",
    "chol = np.linalg.cholesky(X)\n",
    "\n",
    "\n",
    "# make save paths\n",
    "elbos = [compute_elbo(X, Y, pi, beta_means, beta_vars, weights, dist, penalty)]\n",
    "convergence_status = False\n",
    "\n",
    "for i in range(maxiter):\n",
    "    # restart dead components if its early in optimization\n",
    "    if i < max_restart:\n",
    "        restart_components(pi, beta_means, beta_vars, weights)\n",
    "\n",
    "    # update pi and beta\n",
    "    for j in range(100):\n",
    "        pi_diff = update_variational_params(X, Y, pi, beta_means, beta_vars, weights)\n",
    "\n",
    "        # exit inner loop if we converged\n",
    "        if pi_diff < 1e-8:\n",
    "            # enter a new outer loop, need this?\n",
    "            convergence_status = False\n",
    "            break\n",
    "\n",
    "    # update weights\n",
    "    weight_diff = update_weights(X, Y, pi, beta_means, beta_vars, weights, penalty, problem, param_dict)\n",
    "    elbos.append(compute_elbo(X, Y, pi, beta_means, beta_vars, weights, dist, penalty))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('iter {} outer loop elbo: {}, max_weight_diff: {}'.format(\n",
    "            i, (elbos[-1] - elbos[-2]), weight_diff))\n",
    "        \n",
    "        # make plot\n",
    "        active = np.abs(weights).max(0) > 1e-3\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        for k in np.arange(K)[active]:\n",
    "            ax[0].scatter(np.arange(N)[pi[:, k] > 2/N], pi[:, k][pi[:, k] > 2/N], alpha=0.5)\n",
    "\n",
    "        sns.heatmap(weights[:, active], annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    if weight_diff < 1e-8:\n",
    "        print('weight parameter converged')\n",
    "        convergence_status = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make plot\n",
    "active = np.abs(weights).max(0) > 1e-3\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for k in np.arange(K)[active]:\n",
    "    ax[0].scatter(np.arange(N)[pi[:, k] > 2/N], pi[:, k][pi[:, k] > 2/N], alpha=0.5)\n",
    "\n",
    "sns.heatmap(weights[:, active], annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike and slab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ss_weights(active_weight_means, active, pi, prior_activity=0.1, prior_variance=1.0):\n",
    "    old_active_weight_means = active_weight_means.copy()\n",
    "    \n",
    "    K = active_weight_means.shape[1]\n",
    "    weight_means = active_weight_means * active\n",
    "    active_weight_var = prior_variance / (1 + prior_variance)\n",
    "\n",
    "    for k in range(K):\n",
    "        # compute residual\n",
    "        residual = Y - (weight_means) @ (X @ pi).T\n",
    "        # remove effect of kth component from residual\n",
    "        r_k = residual + (weight_means[:, k])[:, None] * (X @ pi[:, k])[None]\n",
    "\n",
    "        # update p(w | s = 1)\n",
    "        active_weight_means[:, k] = active_weight_var * r_k @ pi[:, k]\n",
    "\n",
    "        # now update p(s = 1)\n",
    "        on = r_k @ pi[:, k] * active_weight_means[:, k] \\\n",
    "            - 0.5 * (active_weight_means[:, k]**2 + active_weight_var) + np.log(prior_activity[k])\n",
    "\n",
    "        normalizer = np.log(np.exp(on) + (1-prior_activity[k]))\n",
    "        active[:, k] = np.exp(on - normalizer)\n",
    "        \n",
    "    weight_diff = np.abs(old_active_weight_means - active_weight_means).max()\n",
    "    return weight_diff\n",
    "\n",
    "\n",
    "def update_pi(X, Y, pi, weights):\n",
    "    old_pi = pi.copy()\n",
    "    T, N = Y.shape\n",
    "    K = pi.shape[1]\n",
    "    \n",
    "    for k in range(K):\n",
    "        # compute residual\n",
    "        residual = Y - weights @ (X @ pi).T\n",
    "\n",
    "        # remove effect of kth component from residual\n",
    "        r_k = residual + (weights[:, k])[:, None] * (X @ pi[:, k])[None]\n",
    "\n",
    "        # r_k^T @ Sigma_inv @ (Sigma @ pi) @ (weights * beta)\n",
    "        pi_k = r_k * weights[:, k][:, None]\n",
    "        pi_k = pi_k.sum(0)\n",
    "\n",
    "        # normalize to probabilities\n",
    "        pi_k = np.exp(pi_k - pi_k.max() + 10)\n",
    "        pi_k = pi_k / pi_k.sum()\n",
    "        pi[:, k] = pi_k\n",
    "    \n",
    "    pi_diff = np.abs(pi - old_pi).max()\n",
    "    return pi_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-1*(np.arange(K) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# set hyperparameters #\n",
    "#######################\n",
    "maxiter = 100\n",
    "K = 5\n",
    "prior_activity = np.exp(-1*(np.arange(K) + 1))\n",
    "\n",
    "\n",
    "#######################################\n",
    "# initialize (variational) parameters #\n",
    "#######################################\n",
    "pi = np.random.random((N, K)) + 1\n",
    "pi = pi / pi.sum(0)\n",
    "\n",
    "active_weight_means = np.random.random((T, K)) * 5\n",
    "active = np.ones((T, K))\n",
    "\n",
    "# problem, param_dict = make_problem(N, K, True)\n",
    "\n",
    "#########################\n",
    "# make mvn distribution #\n",
    "#########################\n",
    "\n",
    "dist = multivariate_normal(cov=X)\n",
    "chol = np.linalg.cholesky(X)\n",
    "\n",
    "\n",
    "# make save paths\n",
    "convergence_status = False\n",
    "\n",
    "for i in range(maxiter):\n",
    "    # update pi and beta\n",
    "    for j in range(100):\n",
    "        pi_diff = update_pi(X, Y, pi, active_weight_means * active)\n",
    "\n",
    "        # exit inner loop if we converged\n",
    "        if pi_diff < 1e-8:\n",
    "            # enter a new outer loop, need this?\n",
    "            convergence_status = False\n",
    "            break\n",
    "\n",
    "    # update weights\n",
    "    weight_diff = update_ss_weights(active_weight_means, active, pi, prior_activity)\n",
    "\n",
    "    print('iter {} outer loop elbo: {}, max_weight_diff: {}'.format(\n",
    "        i, (0), weight_diff))\n",
    "\n",
    "    # make plot\n",
    "    active_components = np.abs(active_weight_means * active).max(0) > 1e-3\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    for k in np.arange(K)[active_components]:\n",
    "        ax[0].scatter(np.arange(N)[pi[:, k] > 2/N], pi[:, k][pi[:, k] > 2/N], alpha=0.5)\n",
    "\n",
    "    sns.heatmap((active_weight_means * active)[:, active_components], annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if weight_diff < 1e-8:\n",
    "        print('weight parameter converged')\n",
    "        convergence_status = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on = r_k @ pi[:, k] * active_weight_means[:, k] \\\n",
    "    - 0.5 * (active_weight_means[:, k]**2 + active_weight_vars[:, k]) + np.log(prior_activity[k])\n",
    "\n",
    "normalizer = np.log(np.exp(on) + (1-prior_activity[k]))\n",
    "logp = on - normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.exp(on - normalizer) + np.exp(off - normalizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_active_mean = r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi1 = pi[:, 0]\n",
    "pi2 = pi[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kzz = np.zeros((K, K))\n",
    "for i in range(K):\n",
    "    for j in range(K):\n",
    "        pi1 = pi[:, i]\n",
    "        pi2 = pi[:, j]\n",
    "        Kzz[i, j] = np.sum(np.outer(pi1, pi2) * X)\n",
    "        \n",
    "Kzz = Kzz + np.diag(np.ones(K) - np.diag(Kzz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad1(a, B):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Kzz, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kzz2 = (X @ pi)\n",
    "Kzz2 = Kzz2.T @ Kzz2\n",
    "sns.heatmap(Kzz2, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X[None] * pi.T[..., None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Kzz - Kzz2, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap((X @ pi).T @ (X @ pi), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(pi.T @ pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (weights * beta_means) @ (X @ pi).T\n",
    "pred.shape, pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, T, figsize=(4*T, 3), sharey=True)\n",
    "for t in range(T):\n",
    "    ax[t].scatter(np.arange(N), Y[t], c='k', marker='x', alpha=0.5)\n",
    "    ax[t].scatter(np.arange(N), pred[t], c='r', marker='o', alpha=0.5)\n",
    "    #ax[t].set_title(tissues[t])\n",
    "    ax[t].set_xlabel('position')\n",
    "ax[0].set_ylabel('zscore')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, T, figsize=(4*T, 3), sharex=True, sharey=True)\n",
    "for t in range(T):\n",
    "    ax[t].scatter(pred[t], Y[t], c='k', marker='x', alpha=0.5)\n",
    "    ax[t].set_title('Tissue {}'.format(t))\n",
    "    ax[t].set_xlabel('prediction')\n",
    "ax[0].set_ylabel('zscore')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, active.sum(), figsize=(4*active.sum(), 3))\n",
    "for i, k in enumerate(np.arange(K)[active]):\n",
    "    ax[i].scatter(np.arange(N), pi[:, k], marker='x', c='k')\n",
    "    ax[i].scatter(causal_snps, pi[causal_snps, k], marker='o', c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up problem\n",
    "K = 12\n",
    "pi = np.random.random((N, K))\n",
    "pi = pi / pi.sum(0)\n",
    "\n",
    "beta_means = np.random.random(K)\n",
    "beta_vars = np.ones(K)\n",
    "weights = np.random.random((T, K)) + 1\n",
    "#weights = causal[:, causal_snps]\n",
    "\n",
    "# make optimization problem\n",
    "problem, param_dict = make_problem(N, K, True)\n",
    "\n",
    "# make mvn distribution\n",
    "dist = multivariate_normal(cov=Sigma_reg)\n",
    "chol = np.linalg.cholesky(Sigma_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(weights, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 3.0\n",
    "outer_elbos = [compute_elbo(Y, pi, beta_means, beta_vars, weights, dist, penalty=1)]\n",
    "inner_elbos = [outer_elbos[0]]\n",
    "maxiter = 50\n",
    "max_restart = 10\n",
    "\n",
    "# initialize without l1 penalty\n",
    "for i in range(maxiter):\n",
    "    # restart components if its early in optimization\n",
    "    if i < max_restart:\n",
    "        not_active = np.abs(weights).max(0) < 1e-6\n",
    "        if not_active.sum() > 0:\n",
    "            print('restarting componenents {}'.format(not_active))\n",
    "\n",
    "            restart_freq = (1 / (pi.sum(1) + 0.01))\n",
    "            restart_freq = restart_freq / restart_freq.sum()\n",
    "            for c in np.arange(K)[not_active]:\n",
    "                pi[:, c] = restart_freq\n",
    "                weights[:, c] = (np.random.random(T) + 1)\n",
    "                beta_means[c] = 1.0\n",
    "                beta_vars[c] = 1.0\n",
    "\n",
    "    print('update variational params')\n",
    "    for j in range(100):        \n",
    "        # iteratively update each component\n",
    "        pi_old = pi.copy()\n",
    "        for k in range(K):\n",
    "            # if component is being used, update\n",
    "            if np.abs(weights[:, k]).max() > 1e-5:\n",
    "                update_pi(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k)\n",
    "                update_beta(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k, chol)\n",
    "                 \n",
    "        # compute elbo to test convergence\n",
    "        inner_elbos.append(compute_elbo(Y, pi, beta_means, beta_vars, weights, dist, penalty=1))\n",
    "\n",
    "        # exit loop if we converged\n",
    "        #print('\\tinner loop elbo: {}'.format((inner_elbos[-1] - inner_elbos[-2])))\n",
    "        print('\\tmax pi diff: {}'.format(np.abs(pi - pi_old).max()))\n",
    "        if np.abs(inner_elbos[-1] - inner_elbos[-2]) < 1e-5:\n",
    "            break\n",
    "\n",
    "    # update weights\n",
    "    print('updating weights')\n",
    "    set_params(pi, beta_means, beta_vars, Sigma_reg, penalty, param_dict)\n",
    "    \n",
    "    old_weights = weights.copy()\n",
    "    weights = np.array([solve_w_tissue(Yt, param_dict, problem) for Yt in Y])\n",
    "    \n",
    "    outer_elbos.append(compute_elbo(Y, pi, beta_means, beta_vars, weights, dist, penalty=1))\n",
    "    inner_elbos.append(outer_elbos[-1])\n",
    "\n",
    "    # exit loop if we converged\n",
    "    print('outer loop elbo: {}'.format((outer_elbos[-1] - outer_elbos[-2])))\n",
    "    print('outer loop weight diff: {}'.format(np.mean(np.square(weights - old_weights))))\n",
    "\n",
    "    # make plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    for k in range(K):\n",
    "        ax[0].scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    ax[0].scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "\n",
    "    active = np.abs(weights).max(0) > 1e-6\n",
    "    sns.heatmap(weights[:, active], annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "    plt.show()\n",
    "    \n",
    "    if np.abs(outer_elbos[-1] - outer_elbos[-2]) < 1e-5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_pi(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Y - (beta_means * weights) @ (Sigma @ pi).T\n",
    "\n",
    "# remove effect of kth component from residual\n",
    "r_k = r + ((beta_means[k] * weights[:, k])[:, None] * (Sigma @ pi[:, k])[None])\n",
    "\n",
    "# get relevant pi\n",
    "pi_k = pi[:, k]\n",
    "\n",
    "# now update beta with new pi\n",
    "beta_vars_a = 1 / (1 + np.sum(weights[:, k]**2))\n",
    "beta_means_a = (beta_vars_a) * \\\n",
    "    (Sigma * pi_k).sum(1) @ solve_cholesky(chol, (r_k * weights[:, k][:, None]).sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    a = (Sigma * pi_k).sum(1) @ solve_cholesky(chol, (r_k * weights[:, k][:, None]).sum(0))\n",
    "    b = np.inner(pi_k, (r_k * weights[:, k][:, None]).sum(0))\n",
    "    print(np.isclose(a, b), a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_k, (r_k * weights[:, k][:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute residual\n",
    "r = Y - (beta_means * weights) @ (Sigma_reg @ pi).T\n",
    "\n",
    "# remove effect of kth component from residual\n",
    "r_k = r + ((beta_means[k] * weights[:, k])[:, None] * (Sigma_reg @ pi[:, k])[None])\n",
    "\n",
    "pi_k = r_k * weights[:, k][:, None] * beta_means[k]\n",
    "pi_k = pi_k.sum(0)\n",
    "\n",
    "# compute_likehood under each inducing point\n",
    "#pi_k = np.array(\n",
    "#    [dist.logpdf(r_k - (beta_means[k] * weights[:, k])[:, None] * Sigma[i][None]).sum() for i in range(N)])\n",
    "\n",
    "# normalize to probabilities\n",
    "pi_k = np.exp(pi_k - pi_k.min())\n",
    "#pi_k = pi_k / pi_k.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_means[6], beta_vars[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi[:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for k in range(K):\n",
    "    ax[0].scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "ax[0].scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "\n",
    "sns.heatmap(weights, annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 5.0\n",
    "outer_elbos = [compute_elbo(Y, pi, beta_means, beta_vars, weights, dist, penalty=1)]\n",
    "inner_elbos = [outer_elbos[0]]\n",
    "maxiter = 20\n",
    "\n",
    "# initialize without l1 penalty\n",
    "for i in range(maxiter):\n",
    "    # update SER models\n",
    "    print('update variational params')\n",
    "    for j in range(100):        \n",
    "        # iteratively update each component\n",
    "        for k in range(K):\n",
    "            # if component is being used, update\n",
    "            if np.abs(weights[:, k]).max() > 1e-5:\n",
    "                update_pi(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k, chol)\n",
    "                update_beta(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k, chol)\n",
    "                \n",
    "        # compute elbo to test convergence\n",
    "        inner_elbos.append(compute_elbo(Y, pi, beta_means, beta_vars, weights, dist, penalty=1))\n",
    "\n",
    "        # exit loop if we converged\n",
    "        print('inner loop elbo diff: {}'.format((inner_elbos[-1] - inner_elbos[-2])))\n",
    "        if np.abs(inner_elbos[-1] - inner_elbos[-2]) < 1e-5:\n",
    "            break\n",
    "\n",
    "    # update weights\n",
    "    print('updating weights')\n",
    "    set_params(pi, beta_means, beta_vars, Sigma_reg, penalty, param_dict)\n",
    "    weights = np.array([solve_w_tissue(Yt, param_dict, problem) for Yt in Y])\n",
    "    \n",
    "    outer_elbos.append(compute_elbo(Y, pi, beta_means, beta_vars, weights, dist, penalty=1))\n",
    "    inner_elbos.append(outer_elbos[-1])\n",
    "\n",
    "    # exit loop if we converged\n",
    "    print('outer loop elbo diff: {}'.format((outer_elbos[-1] - outer_elbos[-2])))\n",
    "    if np.abs(outer_elbos[-1] - outer_elbos[-2]) < 1e-5:\n",
    "        break\n",
    "            \n",
    "    # make plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for k in range(K):\n",
    "        ax[0].scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    ax[0].scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    \n",
    "    sns.heatmap(weights, annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # make plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for k in range(K):\n",
    "        ax[0].scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    ax[0].scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    \n",
    "    sns.heatmap(weights, annot=True, cmap='RdBu_r', ax=ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(inner_elbos)), inner_elbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute residual\n",
    "r = Y - (beta_means * weights) @ (Sigma_reg @ pi).T\n",
    "\n",
    "# remove effect of kth component from residual\n",
    "r_k = r + ((beta_means[k] * weights[:, k])[:, None] * (Sigma_reg @ pi[:, k])[None])\n",
    "\n",
    "# compute_likehood under each inducing point\n",
    "pi_k = np.array(\n",
    "    [dist.logpdf(r_k - (beta_means[k] * weights[:, k])[:, None] * Sigma_reg[i][None]).sum() for i in range(N)])\n",
    "\n",
    "# normalize to probabilities\n",
    "pi_k = np.exp(pi_k - pi_k.min())\n",
    "pi_k = pi_k / pi_k.sum()\n",
    "pi_k1 = pi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pi_k1, pi_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_k.shape, weights[:, k].shape, beta_means[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-2 * np.sum(r_k * weights[:, k][:, None] * beta_means[k], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((Sigma * pi_k).sum(1) - (Sigma@pi)[:, 0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Sigma@pi)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_Kxz = (Sigma_reg * pi_k @ Sigma_reg) - \\\n",
    "    ((Sigma_reg * pi_k).sum(1) * (Sigma_reg * pi_k).sum(1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(np.linalg.solve(Sigma_reg, var_Kxz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(1e-7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = 10.0\n",
    "\n",
    "# remove component\n",
    "for i in range(5):\n",
    "    # update SER models\n",
    "    for j in range(5):\n",
    "        for k in range(K):\n",
    "            # if component is being used, update\n",
    "            if ~np.isclose(np.abs(weights[:, k]).max(), 0):\n",
    "                ser_mvn(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k, chol)\n",
    "\n",
    "            elbos.append(compute_elbo(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, penalty=penalty))\n",
    "            print('outer_iter {}, inner_iter {}, component {}:, {}'.format(i, j, k, elbos[-1]))\n",
    "\n",
    "    # update weights\n",
    "    print('updating weights')\n",
    "    set_params(pi, beta_means, beta_vars, Sigma_reg, penalty, param_dict)\n",
    "    weights = np.array([solve_w_tissue(Yt, param_dict, problem) for Yt in Y])\n",
    "    \n",
    "    for k in range(K):\n",
    "        plt.scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(weights, annot=True, cmap='RdBu_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove component\n",
    "for i in range(10):\n",
    "    # update SER models\n",
    "    for j in range(1):\n",
    "        for k in range(K):\n",
    "            print('outer_iter {}, inner_iter {}, component {}'.format(i, j, k))\n",
    "            ser_mvn(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k, chol)\n",
    "\n",
    "    # update weights\n",
    "    print('updating weights')\n",
    "    set_params(pi, beta_means, beta_vars, Sigma_reg, 10.0, param_dict)\n",
    "    weights = np.array([solve_w_tissue(Yt, param_dict, problem) for Yt in Y])\n",
    "    \n",
    "    for k in range(K):\n",
    "        plt.scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    plt.show()\n",
    "    \n",
    "    vmax = np.max(np.abs(weights))\n",
    "    vmin = -vmax\n",
    "    sns.heatmap(weights, annot=True, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove component\n",
    "# update weights\n",
    "print('updating weights')\n",
    "set_params(pi, beta_means, beta_vars, Sigma_reg, 40.0, param_dict)\n",
    "weights = np.array([solve_w_tissue(Yt, param_dict, problem) for Yt in Y])\n",
    "\n",
    "vmax = np.max(np.abs(weights))\n",
    "vmin = -vmax\n",
    "sns.heatmap(weights, annot=True, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    for k in range(K):\n",
    "        print('outer_iter {}, inner_iter {}, component {}'.format(i, j, k))\n",
    "        ser_mvn(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k, chol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = ((Sigma @ pi) @ (beta_means * weights).T)\n",
    "\n",
    "fig, ax = plt.subplots(1, T, figsize=(4*T, 3), sharex=True, sharey=True)\n",
    "for t in range(T):\n",
    "    ax[t].scatter(reconstruction[:, t], Y[t], alpha=0.1, marker='x', c='k')\n",
    "    ax[t].scatter(reconstruction[:, t][causal[t] > 0], Y[t][causal[t] > 0], marker='*', c='r', s=100)\n",
    "    ax[t].set_title('Tissue {}'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = ((Sigma @ pi) @ (beta_means * weights).T)\n",
    "\n",
    "fig, ax = plt.subplots(1, T, figsize=(4*T, 3), sharex=True, sharey=True)\n",
    "for t in range(T):\n",
    "    ax[t].scatter(np.arange(N), np.square(reconstruction[:, t] - Y[t]), alpha=0.1, marker='x', c='k')\n",
    "    ax[t].scatter(np.arange(N)[causal[t] > 0], np.square(reconstruction[:, t][causal[t] > 0] - Y[t][causal[t] > 0]), marker='*', c='r', s=100)\n",
    "    ax[t].set_title('Tissue {}, SE'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pi = T10_5_simulation_param_dict['pi']\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "colors = ['r', 'b', 'g', 'y']\n",
    "for k in range(K):\n",
    "    plt.scatter(np.arange(N)[pi[:, k] > 2/N],\n",
    "                pi[:, k][pi[:, k] > 2/N], alpha=0.5, marker='x', c=colors[k])\n",
    "    plt.scatter(causal_snps[pi[causal_snps, k] > 2/N],\n",
    "                pi[causal_snps, k][pi[causal_snps, k] > 2/N], alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n",
    "\n",
    "#plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='*', c='k', s=100)\n",
    "plt.xlabel('SNP')\n",
    "plt.ylabel('Probability of being inducing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pi = T10_5_simulation_param_dict['pi']\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "colors = ['r', 'b', 'g', 'y']\n",
    "for k in range(4):\n",
    "    plt.scatter(np.arange(N) * ~np.isclose(pi[:, k], 0) ,\n",
    "                pi[:, k] * ~np.isclose(pi[:, k], 0), alpha=0.5, marker='x', c=colors[k])\n",
    "    plt.scatter(causal_snps[~np.isclose(pi[causal_snps, k], 0)],\n",
    "                pi[causal_snps, k][~np.isclose(pi[causal_snps, k], 0)], alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n",
    "\n",
    "#plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='*', c='k', s=100)\n",
    "plt.xlabel('SNP')\n",
    "plt.ylabel('Probability of being inducing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = ((Sigma @ pi) @ (beta_means * weights).T)\n",
    "reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T10_5_simulation_param_dict = {\n",
    "    'weights': weights,\n",
    "    'beta_means': beta_means,\n",
    "    'beta_vars': beta_vars,\n",
    "    'pi': pi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(weights, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem, param_dict = make_problem(N, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pi = T10_5_simulation_param_dict['pi']\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "colors = ['r', 'b', 'g', 'y']\n",
    "for k in range(4):\n",
    "    plt.scatter(np.arange(N) * ~np.isclose(pi[:, k], 0) ,\n",
    "                pi[:, k] * ~np.isclose(pi[:, k], 0), alpha=0.5, marker='x', c=colors[k])\n",
    "    plt.scatter(causal_snps[~np.isclose(pi[causal_snps, k], 0)],\n",
    "                pi[causal_snps, k][~np.isclose(pi[causal_snps, k], 0)], alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n",
    "\n",
    "#plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='*', c='k', s=100)\n",
    "plt.xlabel('SNP')\n",
    "plt.ylabel('Probability of being inducing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up problem\n",
    "K = 3\n",
    "\n",
    "pi_tissue = np.random.random((N, K))\n",
    "pi_tissue = pi / pi.sum(0)\n",
    "\n",
    "tissue_beta_means = np.random.random((K))\n",
    "tissue_beta_vars = np.ones((K))\n",
    "\n",
    "# make mvn distribution\n",
    "dist = multivariate_normal(cov=Sigma_reg)\n",
    "chol = np.linalg.cholesky(Sigma_reg)\n",
    "\n",
    "tissue_weights = np.ones((1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem, param_dict = make_problem(N, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissues = [0]\n",
    "t = 0\n",
    "for i in range(10):\n",
    "    # update SER models\n",
    "    for k in range(K):\n",
    "        print('tissue {}, iter {}, component {}'.format(t, i, k))\n",
    "        ser_mvn(\n",
    "            Y[tissues], pi_tissue,\n",
    "            tissue_beta_means, tissue_beta_vars,\n",
    "            tissue_weights, Sigma_reg, dist, k, chol)\n",
    "\n",
    "    # update weights\n",
    "    set_params(pi_tissue, tissue_beta_means, tissue_beta_vars, Sigma_reg, 0.1, param_dict)\n",
    "    tissue_weights = solve_w_tissue(Y[t], param_dict, problem)[None]\n",
    "    \n",
    "    print(tissue_beta_means, tissue_beta_vars)\n",
    "    print(tissue_weights)\n",
    "    for k in range(K):\n",
    "        plt.scatter(np.arange(N), pi_tissue[:, k], alpha=0.5, marker='x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(N), pi_tissue[:, 1], alpha=0.5, marker='x')\n",
    "#plt.scatter(np.arange(N), pi_tissue[:, 1], alpha=0.5, marker='x')\n",
    "#plt.scatter(np.arange(N), pi_tissue[:, 2], alpha=0.5, marker='x')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_mvn(Y, pi, beta_means, beta_vars, weights, Sigma, dist, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove component\n",
    "for t in [4]:\n",
    "    for i in range(5):\n",
    "        # update SER models\n",
    "        for k in range(K):\n",
    "            print('tissue {}, iter {}, component {}'.format(t, i, k))\n",
    "            ser_mvn(\n",
    "                Y[t][None], pi_tissue[t],\n",
    "                tissue_beta_means[t], tissue_beta_vars[t],\n",
    "                np.ones(K)[None], Sigma_reg, dist, k)\n",
    "        \n",
    "        for k in range(K):\n",
    "            plt.scatter(np.arange(N), pi_tissue[t, :, k], alpha=0.1, marker='x')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_k = pi_tissue[t, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_Kxz = (Sigma * pi_k @ Sigma) - \\\n",
    "    ((Sigma * pi_k).sum(1) * (Sigma * pi_k).sum(1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_Kxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_tissue[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(20, 3), sharey=True)\n",
    "\n",
    "for i, t in enumerate([0, 2, 4, 7, 9]):\n",
    "    pi = pi_tissue[t]\n",
    "    colors = ['r', 'b', 'g', 'y']\n",
    "    for k in range(K):\n",
    "        ax[i].scatter(np.arange(N) * ~np.isclose(pi[:, k], 0) ,\n",
    "                    pi[:, k] * ~np.isclose(pi[:, k], 0), alpha=0.5, marker='x', c=colors[k])\n",
    "        ax[i].scatter(causal_snps[~np.isclose(pi[causal_snps, k], 0)],\n",
    "                    pi[causal_snps, k][~np.isclose(pi[causal_snps, k], 0)], alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n",
    "\n",
    "#plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='*', c='k', s=100)\n",
    "ax[0].set_xlabel('SNP')\n",
    "ax[0].set_ylabel('Probability of being inducing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with correct weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up problem\n",
    "K = 4\n",
    "pi = np.random.random((N, K))\n",
    "pi = pi / pi.sum(0)\n",
    "\n",
    "beta_means = np.random.random(K)\n",
    "beta_vars = np.ones(K)\n",
    "weights = causal[:, causal_snps]\n",
    "\n",
    "# make optimization problem\n",
    "problem, param_dict = make_problem(N, K)\n",
    "\n",
    "# make mvn distribution\n",
    "dist = multivariate_normal(cov=Sigma_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(weights, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove component\n",
    "for i in range(5):\n",
    "    # update SER models\n",
    "    for k in range(K):\n",
    "        print('iter {}, component {}'.format(i, k))\n",
    "        ser_mvn(Y, pi, beta_means, beta_vars, weights, Sigma_reg, dist, k)\n",
    "\n",
    "    # update weights\n",
    "    #set_params(pi, beta_means, beta_vars, Sigma_reg, 0.1, param_dict)\n",
    "    #weights = np.array([solve_w_tissue(Yt, param_dict) for Yt in Y])\n",
    "    \n",
    "    for k in range(K):\n",
    "        plt.scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(weights, annot=True, cmap='Blues')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T10_5_simulation_correct_W_init_param_dict = {\n",
    "    'weights': weights,\n",
    "    'beta_means': beta_means,\n",
    "    'beta_vars': beta_vars,\n",
    "    'pi': pi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(weights, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = T10_5_simulation_correct_W_init_param_dict['pi']\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "colors = ['r', 'b', 'g', 'y']\n",
    "for k in range(4):\n",
    "    plt.scatter(np.arange(N) * ~np.isclose(pi[:, k], 0) ,\n",
    "                pi[:, k] * ~np.isclose(pi[:, k], 0), alpha=0.5, marker='x', c=colors[k])\n",
    "    plt.scatter(causal_snps[~np.isclose(pi[causal_snps, k], 0)],\n",
    "                pi[causal_snps, k][~np.isclose(pi[causal_snps, k], 0)], alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n",
    "\n",
    "#plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='*', c='k', s=100)\n",
    "plt.xlabel('SNP')\n",
    "plt.ylabel('Probability of being inducing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(causal[:, causal_snps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Y - (beta_means * weights) @ (Sigma @ pi).T\n",
    "\n",
    "# remove component\n",
    "for i in range(5):\n",
    "    # update SER models\n",
    "    for k in range(K):\n",
    "        print('iter {}, component {}'.format(i, k))\n",
    "        # remove effect of kth component from residual\n",
    "        r_k = r + ((beta_means[k] * weights[:, k])[:, None] * (Sigma @ pi[:, k])[None])\n",
    "        \n",
    "        # trace term the same across z so dont compute, update pi_k\n",
    "        pi_k = np.array(\n",
    "            [dist.logpdf(r_k - (beta_means[k] * weights[:, k])[:, None] * Sigma_reg[i][None]).sum() for i in range(N)])\n",
    "        pi_k = np.exp(pi_k - pi_k.min())\n",
    "        pi_k = pi_k / pi_k.sum()\n",
    "        pi[:, k] = pi_k\n",
    "        \n",
    "        # now update beta\n",
    "        var_Kxz = (Sigma_reg * pi_k @ Sigma_reg) - ((Sigma_reg * pi_k).sum(1) * (Sigma_reg * pi_k).sum(1)[:, None])\n",
    "        beta_vars[k] = 1 + (weights[:, k]**2).sum() * (1 + np.trace(np.linalg.solve(Sigma_reg, var_Kxz)))\n",
    "        beta_means[k] = (1 / beta_vars[k]) * (Sigma * pi_k).sum(1) @ np.linalg.solve(Sigma_reg, (r_k * weights[:, k][:, None]).sum(0))\n",
    "\n",
    "        # update residual for next iteration\n",
    "        r = Y - (beta_means * weights) @ (Sigma @ pi).T\n",
    "\n",
    "    # update weights\n",
    "    set_params(pi, beta_means, beta_vars, Sigma_reg, 10, param_dict)\n",
    "    weights = np.array([solve_w_tissue(Yt, param_dict) for Yt in Y])\n",
    "    \n",
    "    for k in range(K):\n",
    "        plt.scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "colors = ['r', 'b', 'g', 'y']\n",
    "for k in range(4):\n",
    "    plt.scatter(np.arange(N) * ~np.isclose(pi[:, k], 0) ,\n",
    "                pi[:, k] * ~np.isclose(pi[:, k], 0), alpha=0.5, marker='x', c=colors[k])\n",
    "    plt.scatter(causal_snps[~np.isclose(pi[causal_snps, k], 0)],\n",
    "                pi[causal_snps, k][~np.isclose(pi[causal_snps, k], 0)], alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n",
    "\n",
    "#plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='*', c='k', s=100)\n",
    "plt.xlabel('SNP')\n",
    "plt.ylabel('Probability of being inducing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, T, figsize=(T*4, 3))\n",
    "for t, ax in enumerate(axs):\n",
    "    ax.scatter(np.arange(N), Y[i], alpha=0.1, marker='x', c='k')\n",
    "    ax.scatter(np.arange(N)[causal[t] > 0], Y[t][causal[t] > 0], c='r', marker='o', s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well do we recover single tissue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "pi = np.random.random((N, K))\n",
    "pi = pi / pi.sum(0)\n",
    "\n",
    "beta_means = np.random.random(K)\n",
    "beta_vars = np.ones(K)\n",
    "weights = causal[:, causal_snps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Y - (beta_means * weights) @ (Sigma @ pi).T\n",
    "\n",
    "for i in range(5):\n",
    "    for k in range(K):\n",
    "        print('iter {}, component {}'.format(i, k))\n",
    "        # remove effect of kth component from residual\n",
    "        r_k = r + ((beta_means[k] * weights[:, k])[:, None] * (Sigma @ pi[:, k])[None])\n",
    "        \n",
    "        # trace term the same across z so dont compute, update pi_k\n",
    "        pi_k = np.array(\n",
    "            [dist.logpdf(r_k - (beta_means[k] * weights[:, k])[:, None] * Sigma_reg[i][None]).sum() for i in range(N)])\n",
    "        pi_k = np.exp(pi_k - pi_k.min())\n",
    "        pi_k = pi_k / pi_k.sum()\n",
    "        pi[:, k] = pi_k\n",
    "        \n",
    "        # now update beta\n",
    "        var_Kxz = (Sigma_reg * pi_k @ Sigma_reg) - ((Sigma_reg * pi_k).sum(1) * (Sigma_reg * pi_k).sum(1)[:, None])\n",
    "        beta_vars[k] = 1 + (weights[:, k]**2).sum() * (1 + np.trace(np.linalg.solve(Sigma_reg, var_Kxz)))\n",
    "        beta_means[k] = (1 / beta_vars[k]) * (Sigma * pi_k).sum(1) @ np.linalg.solve(Sigma_reg, (r_k * weights[:, k][:, None]).sum(0))\n",
    "\n",
    "        # update residual for next iteration\n",
    "        r = Y - (beta_means * weights) @ (Sigma @ pi).T\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter(np.arange(N), pi[:, k], alpha=0.5)\n",
    "    plt.scatter(causal_snps, np.zeros_like(causal_snps), marker='|', c='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var = [weights[0, k]**2 * (Sigma * pi[:, k] @ Sigma) * [beta_means[k]**2 + beta_vars[k]] - \n",
    "#       np.outer(pi[:, k], pi[:, k]) * beta_means[k]**2 for k in range(K)]\n",
    "\n",
    "component_vars = [(Sigma * pi[:, k] @ Sigma) * [beta_means[k]**2 + beta_vars[k]] - \n",
    "       np.outer(pi[:, k], pi[:, k]) * beta_means[k]**2 for k in range(K)]\n",
    "traces = [np.trace(np.linalg.solve(Sigma_reg, var)) for var in component_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kxz = (Sigma @ pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_problem(N, K):\n",
    "    weights_t = cvxpy.Variable(K)\n",
    "    _beta_means = cvxpy.Parameter(K)\n",
    "    _beta_vars = cvxpy.Parameter(K, nonneg=True)\n",
    "    _Sigma_inv_Kxz = cvxpy.Parameter((N, K))\n",
    "    _B = cvxpy.Parameter((K, K), PSD=True)  # beta @ Kzx Sigma_inv Kxz beta\n",
    "    _penalty = cvxpy.Parameter((), pos=True)\n",
    "    _data = cvxpy.Parameter(N)\n",
    "\n",
    "\n",
    "    lin = -2 * (_data @ _Sigma_inv_Kxz @ cvxpy.diag(_beta_means)) @ weights_t\n",
    "    quad = cvxpy.quad_form(weights_t, _B)\n",
    "    trace = cvxpy.square(weights_t) @ _beta_vars\n",
    "    l1 = cvxpy.norm1(weights_t)\n",
    "\n",
    "    expression = cvxpy.sum(lin + quad + trace + _penalty * l1)\n",
    "    problem = cvxpy.Problem(cvxpy.Minimize(expression))\n",
    "\n",
    "    param_dict={\n",
    "        '_beta_means': _beta_means,\n",
    "        '_beta_vars': _beta_vars,\n",
    "        '_Sigma_inv_Kxz': _Sigma_inv_Kxz,\n",
    "        '_B': _B,\n",
    "        '_penalty': _penalty,\n",
    "        '_data': _data\n",
    "    }\n",
    "    \n",
    "    return problem, param_dict\n",
    "\n",
    "def set_params(pi, beta_means, beta_vars, Sigma, penalty, param_dict):\n",
    "    Kxz = Sigma @ pi\n",
    "    Sigma_inv_Kxz = np.linalg.solve(Sigma_reg, Kxz)\n",
    "    A = Kxz.T @ Sigma_inv_Kxz\n",
    "    A = A + np.diag(np.diag(np.eye(K) - A))\n",
    "    B = np.diag(beta_means) @ A @ np.diag(beta_means)\n",
    "    \n",
    "    param_dict['_beta_means'].value = beta_means\n",
    "    param_dict['_beta_vars'].value = beta_vars\n",
    "    param_dict['_Sigma_inv_Kxz'].value = Sigma_inv_Kxz\n",
    "    param_dict['_B'].value = B\n",
    "    param_dict['_penalty'].value = penalty\n",
    "    \n",
    "def solve_w_tissue(Yt, param_dict):\n",
    "    param_dict['_data'].value= Yt\n",
    "    problem.solve()\n",
    "    return problem.variables()[0].value\n",
    "\n",
    "\n",
    "problem, param_dict = make_problem(N, K)\n",
    "\n",
    "set_params(pi, beta_means, beta_vars, Sigma_reg, 10, param_dict)\n",
    "weights = np.array([solve_w_tissue(Yt, param_dict) for Yt in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kxz = Sigma @ pi\n",
    "Sigma_inv_Kxz = np.linalg.solve(Sigma_reg, Kxz)\n",
    "A = Kxz.T @ Sigma_inv_Kxz\n",
    "A = A + np.diag(np.diag(np.eye(K) - A))\n",
    "B = np.diag(beta_means) @ A @ np.diag(beta_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_beta_means.value = beta_means\n",
    "_beta_vars.value = beta_vars\n",
    "_Sigma_inv_Kxz.value = Sigma_inv_Kxz\n",
    "_B.value = B\n",
    "_penalty.value = 1\n",
    "_data.value = Y[0]\n",
    "\n",
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.variables()[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_t.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = -2 * (_data @ _Sigma_inv_Kxz @ cvxpy.diag(_beta_means)) @ weights_t\n",
    "quad = cvxpy.quad_form(weights_t, cvxpy.diag(_beta_means) @ Kxz.T @ _Sigma_inv_Kxz @ cvxpy.diag(_beta_means))\n",
    "quad = cvxpy.quad_form(weights_t, cvxpy.diag(_beta_means) @ A @ cvxpy.diag(_beta_means))\n",
    "\n",
    "\n",
    "trace = weights_t @ _beta_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.normal(size=(10))\n",
    "U = np.random.random((10, 10))\n",
    "U = U @ U.T\n",
    "(np.diag(u) @ U @ np.diag(u) - np.diag(u**2) @ U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad + lin + weights_t @ beta_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_t @ beta_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma[:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi[:, 0]**2 * beta_vars[0] + ((beta_means[0]**2 + beta_vars[0]) * pi[:, 0] * (1 - pi[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Sigma[0] * Sigma[10] * pi[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_snps[~np.isclose(pi[causal_snps, k], 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(causal_snps * ~np.isclose(pi[causal_snps, k], 0),\n",
    "                pi[causal_snps, k] * ~np.isclose(pi[causal_snps, k], 0), alpha=0.5, marker='o', c=colors[k], s=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    plt.scatter(np.arange(N), pi[:, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_k = np.array(\n",
    "    [dist.logpdf(r_k - (betas[k] * weights[:, k])[:, None] * Sigma_reg[i][None]).sum() for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.exp(pi_k - pi_k.min())\n",
    "a = a / a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log sum exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "t = 0\n",
    "\n",
    "norm.logpdf(r[t], (Sigma[i] * (betas.shape * weights[t])[:, None])).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, index, inverse = np.unique((Sigma> 0.9), axis=1, return_index=True, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_sub = Sigma[index][:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cholesky(Sigma_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(\n",
    "    norm.logpdf(r[t][:, None, None],\n",
    "    Sigma[:, :, None] * (betas * weights[t])[None, None]).sum(axis=(0, 1))\n",
    "    for t in range(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.logpdf(\n",
    "    r.T[:, :, None],\n",
    "    Sigma[i][:, None, None] * (betas.shape * weights)[None]\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.logpdf(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.logpdf(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = Sigma *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "pi = np.random.random((N, K))\n",
    "pi = pi / pi.sum(0)\n",
    "\n",
    "betas = np.random.random(K)\n",
    "weights = np.random.random((T, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_Sigma = Sigma @ np.delete(pi, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape, exp_Sigma.shape, betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = 0\n",
    "exp_Sigma = Sigma @ np.delete(pi, component, axis=1)\n",
    "residual = Y - np.delete(weights * betas, component, axis=1) @ exp_Sigma.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "Var_z = Sigma * pi @ Sigma.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_z = Sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(Var_z, Sigma @ (pi[:, None] *Sigma.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Sigma * pi).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma @ pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "j = 100\n",
    "\n",
    "(Sigma[i] * Sigma[j] * pi).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_z[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectsize=5\n",
    "chol = np.linalg.cholesky(Sigma + np.eye(N)*1e-6)\n",
    "Y = (effectsize * Sigma @ causal.T + chol @ np.random.normal(size=causal.T.shape)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_data(Y, W, qmu):\n",
    "    Sigj_alpha_j = np.einsum('ij,ji->i', Sigma, qmu)\n",
    "    residuals = []\n",
    "    for tissue in range(W.shape[0]):\n",
    "        tmp = W[tissue] * Sigma[snp] * Sigj_alpha_j\n",
    "        residuals.append(Y[tissue] - tmp.sum() + tmp)\n",
    "    return np.array(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.random((T, N))\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_problem(Sigma, penalty=1.0):\n",
    "    weights = cvxpy.Variable((N))\n",
    "    data = cvxpy.Parameter(N)\n",
    "    slopes = cvxpy.Parameter(N)\n",
    "\n",
    "    mean = Sigma @ cvxpy.diag(alpha_post) @ weights\n",
    "    error = (data - mean)\n",
    "\n",
    "    quad = cvxpy.sum(cvxpy.matrix_frac(error, Sigma + np.eye(N)*1e-6))\n",
    "    trace = 0\n",
    "    norm = cvxpy.norm1(weights)\n",
    "    expression = quad + trace + penalty * norm\n",
    "    \n",
    "    constraints = [weights >= 0]\n",
    "    \n",
    "    problem = cvxpy.Problem(cvxpy.Minimize(expression), constraints)\n",
    "    return problem, data, slopes, weights\n",
    "\n",
    "def get_W(Yt, alpha, problem, data, slopes, weights):\n",
    "    data.value = Yt\n",
    "    slopes.value = alpha\n",
    "    problem.solve(verbose=True)\n",
    "    return weights.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_slopes(W):\n",
    "    precision_post = (W.T @ W * Sigma + (np.eye(N) * gamma))\n",
    "    alpha_post = np.linalg.solve(precision_post, (W * Y).sum(0))\n",
    "    A_post = np.linalg.inv(precision_post)\n",
    "    return alpha_post, A_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem, data, slopes, weights = make_problem(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "W = np.random.random((T, N))\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_post, A_post = update_slopes(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(N), alpha_post)\n",
    "plt.show()\n",
    "plt.scatter(np.arange(N), np.diag(A_post))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(list(map(lambda Yt: get_W(Yt, alpha_post, problem, data, slopes, weights), Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(W, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(causal_snps[None] + np.arange(-10, 10)[:, None]).T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(W[:, (causal_snps[None] + np.arange(-20, 20)[:, None]).T.flatten()], cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cvxpy.Variable((N))\n",
    "data = cvxpy.Parameter(N)\n",
    "slopes = cvxpy.Parameter(N)\n",
    "\n",
    "mean = Sigma @ cvxpy.diag(alpha_post) @ weights\n",
    "error = (data - mean)\n",
    "\n",
    "quad = cvxpy.sum(cvxpy.matrix_frac(error, Sigma + np.eye(N)*1e-6))\n",
    "trace = 0\n",
    "norm = cvxpy.norm1(weights)\n",
    "expression = quad + trace + 0.1 * norm\n",
    "\n",
    "constraints = [weights >= 0]\n",
    "\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(expression), constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma @ cvxpy.diag(alpha_post) @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = Sigma @ cvxpy.diag(alpha_post) @ weights\n",
    "error = (data - mean)\n",
    "\n",
    "quad = cvxpy.sum(cvxpy.matrix_frac(error, Sigma + np.eye(N)*1e-6))\n",
    "trace = 0\n",
    "norm = cvxpy.norm1(weights)\n",
    "expression = quad + trace + 0.1 * norm\n",
    "\n",
    "constraints = [weights >= 0]\n",
    "\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(expression), constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = []\n",
    "slopes.value = alpha_post\n",
    "for tissue in range(T):\n",
    "    data.value = Y[tissue]\n",
    "    problem.solve(verbose=True)\n",
    "    W.append(weights.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = W.max(0) > 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(causal[:, causal_snps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(W)\n",
    "active = W.max(0) > 1e-6\n",
    "sns.heatmap(np.array(W[:, causal_snps])[:, :], cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(W)\n",
    "active = W.max(0) > 1e-6\n",
    "sns.heatmap(np.array(W[:, active])[:, :], cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(N), weights.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(alpha_post * W, W * alpha_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W * alpha_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W @ alpha_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize\n",
    "W = np.random.random((T, N))\n",
    "qmu = np.zeros((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = np.argsort(Sigma[causal_snps]**2, axis=1)[:, -50:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once per weight update\n",
    "total_weight = (W**2).sum(0)\n",
    "denom = np.diag(Sigma @ Sigma.T) * total_weight + 1\n",
    "\n",
    "tmp = []\n",
    "for snp in active:\n",
    "    qvar_snp = np.eye(N) - total_weight[snp]/denom[snp] * np.outer(Sigma[snp], Sigma[snp])\n",
    "    tmp.append(qvar_snp @ Sigma[snp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through snps\n",
    "pseudo_data = make_pseudo_data(Y, W, qmu)\n",
    "agg = (pseudo_data * W).sum(0)\n",
    "\n",
    "for _ in range(10):\n",
    "    for i, snp in enumerate(active):\n",
    "        qmu_snp = (tmp[i] * agg[snp])\n",
    "        qmu[snp] = qmu_snp\n",
    "        pseudo_data = make_pseudo_data(Y, W, qmu)\n",
    "        agg = (pseudo_data * W).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvar_snp = np.eye(N) - total_weight[snp]/denom[snp] * np.outer(Sigma[snp], Sigma[snp])\n",
    "plt.scatter(np.arange(N), np.diag(qvar_snp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[tissue], (W @ qmu)[tissue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(N), qmu[active[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_data(Y, W, qmu, tissue, component):\n",
    "    \"\"\"\n",
    "    make psuedo data for tissue, snp pair\n",
    "    \"\"\"\n",
    "    pseudo_data = Y[:, tissue]\n",
    "    pseudo_data -= (Sigma * (np.einsum('ij, ji->i', Sigma, qmu) * W[tissue])[None]).sum(1)\n",
    "    pseudo_data += W[tissue, component] * Sigma[component] * np.inner(Sigma[component], qmu[component])\n",
    "    return pseudo_data\n",
    "\n",
    "def make_pseudo_data2(Y, W, qmu, component):\n",
    "    \"\"\"\n",
    "    make psuedo data for tissue, snp pair\n",
    "    \"\"\"\n",
    "    pseudo_data = Y.T\n",
    "    pseudo_data -= W * Sigma * np.einsum('ij, ji->i', Sigma, qmu)[None]\n",
    "    pseudo_data += W[component] * Sigma[component] * np.inner(Sigma[component], qmu[component])\n",
    "    return pseudo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = Y.T\n",
    "pseudo_data -= W * Sigma * np.einsum('ij, ji->i', Sigma, qmu)[None]\n",
    "pseudo_data += W[component] * Sigma[component] * np.inner(Sigma[component], qmu[component])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W[..., None] * Sigma * np.einsum('ij, ji->i', Sigma, qmu)[None]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = Y[:, tissue]\n",
    "pseudo_data -= (Sigma * (np.einsum('ij, ji->i', Sigma, qmu) * W[tissue])[None]).sum(1)\n",
    "pseudo_data += W[tissue, component] * Sigma[component] * np.inner(Sigma[component], qmu[component])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = Y.T\n",
    "pseudo_data -= W * np.einsum('ij, ji->i', Sigma, qmu)[None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "residual = (W[..., None] * (Sigma * np.einsum('ij, ji->i', Sigma, qmu)[None])[None]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue = 0\n",
    "component = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = Y[:, tissue]\n",
    "pseudo_data -= W[tissue] * np.einsum('ij, ji->i', Sigma, qmu)\n",
    "pseudo_data += W[tissue, component] * Sigma[component] * np.inner(Sigma[component], qmu[component])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pseudo_data(Y[:, 0], W[0, 10], qmu, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=500\n",
    "Sigma_i = Sigma[i][:, None]\n",
    "\n",
    "def update_variational_approximation_component(Y, Sigma, W, component):\n",
    "    denom = np.diag(Sigma + Sigma @ Sigma.T)\n",
    "    total_weight = (W**2).sum(0)\n",
    "    pseudo_data = make_pseudo_data(Y, Sigma, W, qmu, tissue, component)\n",
    "    Sigma_i = Sigma[component][:, None]\n",
    "    \n",
    "    posterior_variance = np.ones(N) - total_weight[component] *(Sigma_i @ Sigma_i.T) / denom[component]\n",
    "    posterior_mean = posterior_variance @ Sigma_i * pseudo_data\n",
    "    return posterior_mean, posterior_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(N), np.diag(posterior_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(Sigma @ np.diag(1/denom) @ Sigma).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_variance.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol = np.linalg.cholesky(Sigma + np.eye(N)*1e-3)\n",
    "Y_iid = 10 * Sigma @ causal.T + np.random.normal(size=causal.T.shape)\n",
    "Y_sigma = 10 * Sigma @ causal.T + chol @ np.random.normal(size=causal.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_model import update_means, precompute, make_pseudo_data, update_mean_component\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 20\n",
    "W = np.random.random((T, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = Sigma**2\n",
    "clustering = SpectralClustering(n_clusters=Q, affinity='precomputed', assign_labels=\"discretize\").fit(R2)\n",
    "indices = [np.arange(N)[clustering.labels_ == i] for i in range(Q)]\n",
    "#indices = [np.arange(N)[::5] for i in range(Q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma[:, [0, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = [0]\n",
    "thresh = 0.5\n",
    "for snp in np.random.permutation(N):\n",
    "    if np.abs(Sigma[snp, active]).max() < thresh:\n",
    "        active.append(snp)\n",
    "active = np.array(active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.abs(Sigma - np.eye(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.ones(N) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "active = (np.ones(N) == 1)\n",
    "for _ in range(10):\n",
    "    for snp in np.random.permutation(np.arange(N)[active]):\n",
    "        if A[snp, active].max() > thresh:\n",
    "            active[snp] = False\n",
    "local_indices = [[a] for a in np.arange(N)[active]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(N)[active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster SNPs, make indices\n",
    "Zs = [X[idx] if len(idx) > 0 else X[::100] for idx in local_indices]\n",
    "Q = len(local_indices)\n",
    "\n",
    "# set up kernel, initialize parameters\n",
    "kernel = MultipleLinearKernel(np.random.random((T, Q))*5, Zs, kernel='fic')\n",
    "precompute = _make_covariances(kernel, X, Zs)\n",
    "\n",
    "q_gmu = np.zeros((N, Q))\n",
    "q_gvar = np.repeat(np.eye(N)[None, ...], Q, axis=0) * 1.0\n",
    "\n",
    "q_gmu_z = [np.zeros((Z.shape[0])) for Z in Zs]\n",
    "q_gvar_z = [np.eye(Z.shape[0]) for Z in Zs]\n",
    "\n",
    "# generate data\n",
    "Y = effectsize * Sigma @ causal.T + np.linalg.cholesky(Sigma + np.eye(N)*1e-6) @ np.random.normal(size=causal.T.shape)\n",
    "\n",
    "reg = 1e-6\n",
    "S = (Sigma + np.eye(N)*reg) / (1 + reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print('updating variational params')\n",
    "    q_gmu_z, q_gvar_z, q_gmu, q_gvar = update_variational_params_inducing(\n",
    "        kernel.W, precompute, Y, S, q_gmu_z, q_gvar_z, niter=10)\n",
    "\n",
    "    print('updating W')\n",
    "    kernel.W = np.array(update_W(Y, q_gmu, q_gvar, S, penalty=penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('updating W')\n",
    "kernel.W = np.array(update_W_se(Y, q_gmu, q_gvar, S, penalty=penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.zeros((N, Q))\n",
    "means = update_means(pre, Y_sigma, W, chol, means, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(kernel.W, square=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_g(q_gmu, kernel.W, top=causal_snps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cvxpy.Variable(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight = (1 + cvxpy.sum_squares(weights))\n",
    "error = (Y_sigma[:, 0] - (means @ weights))\n",
    "quad = cvxpy.matrix_frac(error, Sigma + np.eye(N)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_file = './simulation_scripts/full_rank_noise_iid_model_globalp_params'\n",
    "q_gmu, q_gvar, W = pickle.load(open(param_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_map = [0, 1, 2, 3]\n",
    "component_map = [2, 1, 3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 10))\n",
    "\n",
    "bs = np.arange(X.shape[0])[np.all(causal == 0, 0)]\n",
    "#sns.heatmap(causal[:, causal_snps], cmap='Blues', square=False, cbar=False, ax=ax[0])\n",
    "sns.heatmap(W[:, component_map], annot=False, cmap='Blues', square=False, cbar=False, ax=ax)\n",
    "ax.set_title('Causal SNP Assignments')\n",
    "ax.set_ylabel('Tissue')\n",
    "ax.set_xlabel('Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, causal_snps.size, figsize=(4*causal_snps.size, 3))\n",
    "for i, cs in enumerate(causal_snps):\n",
    "    ax[i].scatter(np.arange(N), Sigma[cs], c=Sigma[cs]**2)\n",
    "    ax[i].scatter(cs, 1, c='r', marker='*', s=100)\n",
    "    ax[i].set_title('Casual SNP {}'.format(cs))\n",
    "    ax[i].set_xlabel('SNP')\n",
    "\n",
    "ax[0].set_ylabel('correlation with causal snp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_g(q_gmu[:, component_map], W, top=causal_snps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmu = (W_inferred @ B_mean_inferred @ X.T)\n",
    "\n",
    "fig, ax = plt.subplots(1, T, figsize=(T*4, 3))\n",
    "for i in range(T):\n",
    "    ax[i].scatter(np.arange(N), Y[:, i], c='k', marker='x', alpha=0.1)\n",
    "    ax[i].scatter(np.arange(N), fmu[i])\n",
    "    ax[i].set_title('Tissue {}'.format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, T, figsize=(T*4, 3), sharex=True, sharey=True)\n",
    "for i in range(T):\n",
    "    ax[i].scatter(fmu[i], Y[:, i], c='k', marker='x', alpha=0.1)\n",
    "    ax[i].set_title('Tissue {}'.format(i))\n",
    "    ax[i].set_xlabel('prediction')\n",
    "ax[0].set_ylabel('observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With  GP version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_linear import update_variational_params_inducing, update_variational_params_inducing_iid\n",
    "from linear_linear import update_W, update_W_iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "from gpflow.multioutput import kernels as mk\n",
    "from gpflow.multioutput import features as mf\n",
    "from linear_linear import update_variational_params_inducing_iid, update_W_iid\n",
    "from linear_linear import update_W_diagonal_sparse_columns, _make_covariances\n",
    "from linear_linear import update_variational_params_inducing, update_W\n",
    "from plots import plot_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniformly distributed inducing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_W(kernel, X, Y, q_gmu, q_gvar, Sigma, 10)\n",
    "\n",
    "active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization via inducing points\n",
    "\n",
    "Check if localization helps in iid noise, sigma noise and full rank noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Sigma + np.eye(N)\n",
    "S = (S / np.sqrt(np.diag(S))) / np.sqrt(np.diag(S))[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_iid = 10 * Sigma @ causal.T + np.random.normal(size=causal.T.shape)\n",
    "Y_full_rank = 10 * Sigma @ causal.T + np.linalg.cholesky(S) @ np.random.normal(size=causal.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 20\n",
    "QQ = 20\n",
    "\n",
    "R2 = Sigma**2\n",
    "clustering = SpectralClustering(\n",
    "    n_clusters=QQ, affinity='precomputed', assign_labels=\"discretize\").fit(R2)\n",
    "#indices = [np.arange(N)[clustering.labels_ == i] for i in clustering.labels_[causal_snps]]\n",
    "indices = [np.arange(N)[clustering.labels_ == i] for i in range(Q)]\n",
    "\n",
    "\n",
    "#indices = [np.union1d(np.arange(N)[::50], idx) for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.labels_[causal_snps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(N), (Sigma @causal.T)[:, 0], c=clustering.labels_)\n",
    "plt.scatter(causal_snps, np.ones_like(causal_snps), c='r', marker='*', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.reset_default_graph_and_session()\n",
    "\n",
    "T = Y_iid.shape[1]\n",
    "Y = Y_full_rank\n",
    "# set up kernel\n",
    "kernel_gen = lambda: gpflow.kernels.Linear(D)\n",
    "kern_list = [kernel_gen() for _ in range(Q)]\n",
    "W = np.random.random((T, Q))*5\n",
    "\n",
    "with gpflow.defer_build():\n",
    "    kernel = mk.SeparateMixedMok(kern_list, W=W) # Notice that we initialise the mixing matrix W\n",
    "    kernel.W.prior = gpflow.priors.Exponential(1)\n",
    "\n",
    "    # set up features\n",
    "    inducing_idx = np.arange(N)[::5]\n",
    "    M = inducing_idx.size\n",
    "    feature = mf.MixedKernelSharedMof(gpflow.features.InducingPoints(X[inducing_idx]))\n",
    "\n",
    "    # initialise mean of variational posterior to be of shape MxL\n",
    "    q_mu = np.zeros((M, Q)) \n",
    "    # initialise \\sqrt() of variational posterior to be of shape LxMxM\n",
    "    q_sqrt = np.repeat(np.eye(M)[None, ...], Q, axis=0) * 1.0\n",
    "\n",
    "    # create SVGP model as usual and optimize\n",
    "    model = gpflow.models.SVGP(\n",
    "        X, Y, kernel, gpflow.likelihoods.Gaussian(),\n",
    "        feat=feature, q_mu=q_mu, q_sqrt=q_sqrt, name='model', whiten=False)\n",
    "\n",
    "model.compile()\n",
    "model.feature.set_trainable(False)\n",
    "model.q_mu.set_trainable(False)\n",
    "model.q_sqrt.set_trainable(False)\n",
    "model.kern.W.set_trainable(False)\n",
    "model.likelihood.set_trainable(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variational parameters\n",
    "precompute = _make_covariances(kernel, X, indices)\n",
    "\n",
    "kernel.W = W\n",
    "q_gmu = np.zeros((N, Q))\n",
    "q_gvar = np.repeat(np.eye(N)[None, ...], Q, axis=0) * 1.0\n",
    "\n",
    "q_gmu_z = [q_gmu[idx, i] for i, idx in enumerate(indices)]\n",
    "q_gvar_z = [q_gvar[i, idx][:, idx] for i, idx in enumerate(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    q_gmu_z, q_gvar_z, q_gmu, q_gvar = update_variational_params_inducing_iid(\n",
    "        model, X, Y, indices, model.likelihood.variance.value,\n",
    "        q_gmu_z, q_gvar_z, niter=10, restart=False, precompute=precompute)\n",
    "\n",
    "    fig = plot_g(q_gmu, kernel.W.value, indices, causal_snps)\n",
    "    plt.show()\n",
    "\n",
    "    update_W_iid(kernel, X, Y, q_gmu, q_gvar, penalty=100)\n",
    "\n",
    "    active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "    sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 10))\n",
    "\n",
    "bs = np.arange(X.shape[0])[np.all(causal == 0, 0)]\n",
    "#sns.heatmap(causal[:, causal_snps], cmap='Blues', square=False, cbar=False, ax=ax[0])\n",
    "active = np.arange(Q)[kernel.W.value.max(0) > 0.1]\n",
    "sns.heatmap(kernel.W.value[:, active], annot=False, cmap='Blues', square=False, cbar=False, ax=ax)\n",
    "ax.set_title('Causal SNP Assignments')\n",
    "ax.set_ylabel('Tissue')\n",
    "ax.set_xlabel('Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[8, 10, 17, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, causal_snps.size, figsize=(4*causal_snps.size, 3))\n",
    "for i, cs in enumerate(causal_snps):\n",
    "    ax[i].scatter(np.arange(N), Sigma[cs], c=Sigma[cs]**2)\n",
    "    ax[i].scatter(cs, 1, c='r', marker='*', s=100)\n",
    "    ax[i].set_title('Casual SNP {}'.format(cs))\n",
    "    ax[i].set_xlabel('SNP')\n",
    "\n",
    "ax[0].set_ylabel('correlation with causal snp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_g(q_gmu[:, [8, 10, 17, 3]], kernel.W.value, np.array(indices)[[8, 10, 17, 3]], top=causal_snps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name='iid_noise_iid_model_local_Q20'\n",
    "pickle.dump((q_gmu, q_gvar, kernel.W.value), open('./simulation_scripts/{}_params'.format(run_name), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute = _make_covariances(kernel, X, indices)\n",
    "\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    # updage variational params\n",
    "    q_gmu_z, q_gvar_z, q_gmu, q_gvar = update_variational_params_inducing_iid(\n",
    "        model, X, Y, indices, model.likelihood.variance.value,\n",
    "        q_gmu_z, q_gvar_z, niter=10, restart=False, precompute=precompute)\n",
    "\n",
    "    fig = plot_g(q_gmu, kernel.W.value, indices, causal_snps)\n",
    "    plt.show()\n",
    "    \n",
    "    #update weights\n",
    "    update_W_diagonal_sparse_columns(kernel, X, Y, q_gmu, q_gvar, entry_penalty=10, column_penalty=0)\n",
    "    \n",
    "    active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "    sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.q_mu = q_gmu[inducing_idx]\n",
    "model.q_sqrt = np.array([np.linalg.cholesky(A[inducing_idx][:, inducing_idx] + np.eye(inducing_idx.size)*1e-6) for A in q_gvar])\n",
    "print(model.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(idx) for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_g(q_gmu, kernel, indices, causal_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = [model.compute_log_likelihood()] \n",
    "for _ in range(20):\n",
    "    # updage variational params\n",
    "    q_gmu_z, q_gvar_z, q_gmu, q_gvar = update_variational_params_inducing_iid(\n",
    "        model, X, Y, indices, model.likelihood.variance.value,\n",
    "        q_gmu_z, q_gvar_z, niter=20, idx=inducing_idx, restart=True, precompute=precompute)\n",
    "\n",
    "    model.q_mu = q_gmu[inducing_idx]\n",
    "    model.q_sqrt = np.array([np.linalg.cholesky(A[inducing_idx][:, inducing_idx] + np.eye(inducing_idx.size)*1e-6) for A in q_gvar])\n",
    "    lls.append(model.compute_log_likelihood())\n",
    "    print(lls[-1])\n",
    "\n",
    "    #update weights\n",
    "    update_W_diagonal_sparse_columns(kernel, X, Y, q_gmu, q_gvar, entry_penalty=10, column_penalty=0)\n",
    "    lls.append(model.compute_log_likelihood())\n",
    "    print(lls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_g(q_gmu, kernel, indices, causal_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = np.arange(QQ)[kernel.W.value.sum(0) > 0.1]\n",
    "sns.heatmap(kernel.W.value[:, active].T, cmap='Blues', square=True, annot=True, cbar=False, yticklabels=active)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.W = causal[:, causal_snps]\n",
    "\n",
    "# updage variational params\n",
    "q_gmu_z, q_gvar_z, q_gmu, q_gvar = update_variational_params_inducing_iid(\n",
    "    model, X, Y, indices, model.likelihood.variance.value,\n",
    "    q_gmu_z, q_gvar_z, niter=20, idx=inducing_idx, restart=True, precompute=precompute)\n",
    "\n",
    "model.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.W = causal[:, causal_snps]/effectsize\n",
    "\n",
    "# updage variational params\n",
    "q_gmu_z, q_gvar_z, q_gmu, q_gvar = update_variational_params_inducing_iid(\n",
    "    model, X, Y, indices, model.likelihood.variance.value,\n",
    "    q_gmu_z, q_gvar_z, niter=20, idx=inducing_idx, restart=True, precompute=precompute)\n",
    "\n",
    "model.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmu = (kernel.W.value @ q_gmu.T)\n",
    "\n",
    "fig, ax = plt.subplots(1, T, figsize=(T*4, 3))\n",
    "for i in range(T):\n",
    "    ax[i].scatter(np.arange(N), Y[:, i], c='k', marker='x', alpha=0.1)\n",
    "    ax[i].scatter(np.arange(N), fmu[i])\n",
    "    ax[i].set_title('Tissue {}'.format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, T, figsize=(T*4, 3), sharex=True, sharey=True)\n",
    "for i in range(T):\n",
    "    ax[i].scatter(fmu[i], Y[:, i], c='k', marker='x', alpha=0.1)\n",
    "    ax[i].set_title('Tissue {}'.format(i))\n",
    "    ax[i].set_xlabel('prediction')\n",
    "ax[0].set_ylabel('observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.reset_default_graph_and_session()\n",
    "S = np.eye(N)\n",
    "\n",
    "# set up kernel\n",
    "kernel_gen = lambda: gpflow.kernels.Linear(D) # gpflow.kernels.SquaredExponential(D) #+ gpflow.kernels.Linear(D)\n",
    "kern_list = [kernel_gen() for _ in range(Q)]\n",
    "W_true = np.concatenate([np.eye(T), np.ones(T)[:, None], np.zeros([T, Q-causal_snps.size])], 1)\n",
    "W = np.random.random((T, Q))*5 + 1\n",
    "\n",
    "with gpflow.defer_build():\n",
    "    kernel = mk.SeparateMixedMok(kern_list, W=W) # Notice that we initialise the mixing matrix W\n",
    "    kernel.W.prior = gpflow.priors.Exponential(1)\n",
    "\n",
    "    # set up features\n",
    "    inducing_idx = np.arange(N)[::5]\n",
    "    M = inducing_idx.size\n",
    "    feature = mf.MixedKernelSharedMof(gpflow.features.InducingPoints(X[inducing_idx]))\n",
    "\n",
    "    # initialise mean of variational posterior to be of shape MxL\n",
    "    q_mu = np.zeros((M, Q)) \n",
    "    # initialise \\sqrt() of variational posterior to be of shape LxMxM\n",
    "    q_sqrt = np.repeat(np.eye(M)[None, ...], Q, axis=0) * 1.0\n",
    "\n",
    "    # create SVGP model as usual and optimize\n",
    "    model = gpflow.models.SVGP(\n",
    "        X, Y, kernel, gpflow.likelihoods.Gaussian(),\n",
    "        feat=feature, q_mu=q_mu, q_sqrt=q_sqrt, name='model', whiten=False)\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model.feature.set_trainable(False)\n",
    "model.q_mu.set_trainable(False)\n",
    "model.q_sqrt.set_trainable(False)\n",
    "model.kern.W.set_trainable(False)\n",
    "model.likelihood.set_trainable(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E step\n",
    "#lls = [model.compute_log_likelihood()]\n",
    "\n",
    "for _ in range(30):\n",
    "    q_gmu, q_gvar = update_variational_params_iid(model, X, Y, S, q_gmu, q_gvar, niter=10, idx=inducing_idx)\n",
    "    lls.append(model.compute_log_likelihood())\n",
    "    \n",
    "    print('v: {}'.format(lls[-1]))\n",
    "    plot_g(q_gmu, kernel, bottom=indices, top=causal_snps, thresh=0.1)\n",
    "    plot_tissues(model, X, Sigma, {})\n",
    "    \n",
    "    # M step-- update weights\n",
    "    update_W_diagonal(kernel, X, Y, S, q_gmu, q_gvar, penalty=1.0)\n",
    "    lls.append(model.compute_log_likelihood())\n",
    "    print('w: {}'.format(lls[-1]))\n",
    "    \n",
    "    active = np.arange(Q)[kernel.W.value.sum(0) > 0.1]\n",
    "    sns.heatmap(kernel.W.value[:, active], **heatmap_kwargs, xticklabels=active)\n",
    "    plt.show()\n",
    "    \n",
    "    model.likelihood.variance = update_variance(kernel, q_gmu, q_gvar)\n",
    "    lls.append(model.compute_log_likelihood())\n",
    "    print('l: {}'.format(lls[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, Q, D, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.reset_default_graph_and_session()\n",
    "\n",
    "#chol = np.linalg.cholesky(Sigma + np.eye(N)*1e-2)\n",
    "#Yrot = np.linalg.solve(chol, Y)\n",
    "S = Sigma + np.eye(N)\n",
    "\n",
    "# set up kernel\n",
    "kernel_gen = lambda: gpflow.kernels.Linear(D) # gpflow.kernels.SquaredExponential(D) #+ gpflow.kernels.Linear(D)\n",
    "kern_list = [kernel_gen() for _ in range(Q)]\n",
    "W_true = np.concatenate([np.eye(T), np.ones(T)[:, None], np.zeros([T, Q-causal_snps.size])], 1)\n",
    "W = np.random.random((T, Q))*5 + 1\n",
    "\n",
    "with gpflow.defer_build():\n",
    "    kernel = mk.SeparateMixedMok(kern_list, W=W) # Notice that we initialise the mixing matrix W\n",
    "    kernel.W.prior = gpflow.priors.Exponential(1)\n",
    "\n",
    "    # set up features\n",
    "    inducing_idx = np.arange(N)[::5]\n",
    "    M = inducing_idx.size\n",
    "    feature = mf.MixedKernelSharedMof(gpflow.features.InducingPoints(X[inducing_idx]))\n",
    "\n",
    "    # initialise mean of variational posterior to be of shape MxL\n",
    "    q_mu = np.zeros((M, Q)) \n",
    "    # initialise \\sqrt() of variational posterior to be of shape LxMxM\n",
    "    q_sqrt = np.repeat(np.eye(M)[None, ...], Q, axis=0) * 1.0\n",
    "\n",
    "    # create SVGP model as usual and optimize\n",
    "    model = gpflow.models.SVGP(\n",
    "        X, Y, kernel, MultivariateGaussian(S),\n",
    "        feat=feature, q_mu=q_mu, q_sqrt=q_sqrt, name='model', whiten=False)\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model.feature.set_trainable(False)\n",
    "model.q_mu.set_trainable(False)\n",
    "model.q_sqrt.set_trainable(False)\n",
    "model.kern.W.set_trainable(False)\n",
    "model.likelihood.set_trainable(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inferred axes:\")\n",
    "print(W_mean_inferred)\n",
    "print(\"Standard Deviation:\")\n",
    "print(W_stddv_inferred)\n",
    "\n",
    "plt.plot(range(1, num_epochs, 5), t)\n",
    "plt.show()\n",
    "\n",
    "with ed.interception(ed.make_value_setter(w=w_mean_inferred,\n",
    "                                          z=z_mean_inferred)):\n",
    "  generate = probabilistic_pca(\n",
    "      data_dim=data_dim, latent_dim=latent_dim,\n",
    "      num_datapoints=num_datapoints, stddv_datapoints=stddv_datapoints)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  x_generated, _ = sess.run(generate)\n",
    "\n",
    "plt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1, label='Actual data')\n",
    "plt.scatter(x_generated[0, :], x_generated[1, :], color='red', alpha=0.1, label='Simulated data (VI)')\n",
    "plt.legend()\n",
    "plt.axis([-20, 20, -20, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disaster_count_model_switch():\n",
    "  early_disaster_rate = ed.Exponential(rate=1., name='early_disaster_rate')\n",
    "  late_disaster_rate = ed.Exponential(rate=1., name='late_disaster_rate')\n",
    "  switchpoint = ed.Uniform(low=0., high=tf.to_float(len(years)),\n",
    "                           name='switchpoint')\n",
    "  def disaster_rate(ys):\n",
    "    return [tf.where(y < switchpoint, early_disaster_rate, late_disaster_rate)\n",
    "            for y in ys]\n",
    "  disaster_count = ed.Poisson(rate=disaster_rate(np.arange(len(years))),\n",
    "                              name='disaster_count')\n",
    "  return disaster_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
